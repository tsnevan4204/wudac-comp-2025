{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f6a05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "PyTorch Product Category Recommender based on Co-Purchase Patterns\n",
    "\n",
    "This script trains a model to recommend product categories based on co-purchase\n",
    "patterns identified within user site sessions. It uses purchase data from the\n",
    "transactions file, identifying co-purchases by grouping transactions that share\n",
    "the same machine_id and site_session_id.\n",
    "\"\"\"\n",
    "\n",
    "# --- Configuration: Define Dataset Sources ---\n",
    "# IMPORTANT: This model uses transactions.csv to find co-purchase pairs within sessions.\n",
    "#            The necessary identifiers (machine_id, site_session_id, prod_category_id)\n",
    "#            are expected in this file. sessions.csv provides context but isn't\n",
    "#            directly used for generating the co-purchase pairs in this specific model.\n",
    "TRANSACTIONS_FILE = 'transactions.csv' # MODIFY THIS: Path to your transactions CSV file (MUST contain machine_id, site_session_id, prod_category_id)\n",
    "CATEGORIES_FILE = 'product_categories.csv' # MODIFY THIS: Path to your product categories CSV file (optional, for names)\n",
    "# SESSIONS_FILE = 'sessions.csv' # Path to sessions.csv (Not directly used in this script's core logic, but good to note)\n",
    "\n",
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from itertools import permutations\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings # To manage warnings more gracefully\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- Global Settings & Hyperparameters ---\n",
    "TEST_SET_SIZE = 0.2  # 20% of sessions for testing\n",
    "RANDOM_STATE = 42    # For reproducible splits\n",
    "RECOMMENDATION_COUNT = 2 # Number of categories to recommend\n",
    "\n",
    "# Model Hyperparameters\n",
    "EMBEDDING_DIM = 64     # Size of the category embedding vectors\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 1024     # Adjust based on memory\n",
    "NUM_EPOCHS = 5        # Adjust based on convergence and dataset size\n",
    "NEGATIVE_SAMPLES_PER_POSITIVE = 5 # How many negatives to sample for each positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa39fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "PyTorch Product Category Recommender based on Co-Purchase Patterns\n",
    "and Machine ID Context.\n",
    "\n",
    "This script trains a model to recommend product categories based on co-purchase\n",
    "patterns identified within user site sessions, incorporating the machine ID\n",
    "as an additional context feature. It uses purchase data from the transactions\n",
    "file, identifying co-purchases by grouping transactions that share the same\n",
    "machine_id and site_session_id. The machine ID is used to potentially learn\n",
    "machine-specific purchase behaviors.\n",
    "\"\"\"\n",
    "\n",
    "# --- Configuration: Define Dataset Sources ---\n",
    "# IMPORTANT: This model uses transactions.csv to find co-purchase pairs within sessions.\n",
    "#            The necessary identifiers (machine_id, site_session_id, prod_category_id)\n",
    "#            are expected in this file.\n",
    "TRANSACTIONS_FILE = 'transactions.csv' # MODIFY THIS: Path to your transactions CSV file (MUST contain machine_id, site_session_id, prod_category_id)\n",
    "CATEGORIES_FILE = 'product_categories.csv' # MODIFY THIS: Path to your product categories CSV file (optional, for names)\n",
    "# SESSIONS_FILE = 'sessions.csv' # Path to sessions.csv (Not directly used in this script's core logic, but good to note)\n",
    "\n",
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from itertools import permutations\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings # To manage warnings more gracefully\n",
    "import random # For picking a sample machine ID\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- Global Settings & Hyperparameters ---\n",
    "TEST_SET_SIZE = 0.2  # 20% of sessions for testing\n",
    "RANDOM_STATE = 42    # For reproducible splits\n",
    "RECOMMENDATION_COUNT = 3 # Number of categories to recommend\n",
    "\n",
    "# Model Hyperparameters\n",
    "EMBEDDING_DIM = 64     # Size of the category and machine embedding vectors\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 1024      # Adjust based on memory\n",
    "NUM_EPOCHS = 5         # Adjust based on convergence and dataset size\n",
    "NEGATIVE_SAMPLES_PER_POSITIVE = 5 # How many negatives to sample for each positive\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def load_data(transactions_file, categories_file):\n",
    "    \"\"\"\n",
    "    Loads purchase transaction data, category names, and creates mappings\n",
    "    for categories and machines.\n",
    "    Assumes transactions_file contains machine_id, site_session_id, and prod_category_id.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data...\")\n",
    "    print(f\"  Transactions file: {transactions_file}\")\n",
    "    print(f\"  Categories file: {categories_file}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    if not os.path.exists(transactions_file):\n",
    "        raise FileNotFoundError(f\"Error: Transactions file not found at {transactions_file}\")\n",
    "\n",
    "    # --- Load Transactions ---\n",
    "    required_cols = ['machine_id', 'site_session_id', 'prod_category_id']\n",
    "    try:\n",
    "        # Use iterator and chunking for potentially large files\n",
    "        chunk_iter = pd.read_csv(\n",
    "            transactions_file,\n",
    "            usecols=required_cols,\n",
    "            dtype={'prod_category_id': 'Int64', # Use nullable integer type first\n",
    "                   'machine_id': 'Int64',      # Assuming these can be large integers\n",
    "                   'site_session_id': 'Int64'},\n",
    "            chunksize=1000000 # Process in chunks of 1 million rows\n",
    "        )\n",
    "\n",
    "        df_trans_list = []\n",
    "        print(\"  Processing transaction file in chunks...\")\n",
    "        for i, chunk in enumerate(chunk_iter):\n",
    "            # Drop rows where essential IDs are missing within the chunk\n",
    "            chunk.dropna(subset=required_cols, inplace=True)\n",
    "            # Ensure IDs are standard integers AFTER dropping NA\n",
    "            if not chunk.empty:\n",
    "                 # Use numpy int64 for consistency and potential compatibility\n",
    "                chunk['prod_category_id'] = chunk['prod_category_id'].astype(np.int64)\n",
    "                chunk['machine_id'] = chunk['machine_id'].astype(np.int64)\n",
    "                chunk['site_session_id'] = chunk['site_session_id'].astype(np.int64)\n",
    "                df_trans_list.append(chunk)\n",
    "            print(f\"    Processed chunk {i+1}...\")\n",
    "\n",
    "        if not df_trans_list:\n",
    "            raise ValueError(f\"No valid data found in {transactions_file} after checking required columns.\")\n",
    "\n",
    "        df_trans = pd.concat(df_trans_list, ignore_index=True)\n",
    "        print(f\"Loaded {len(df_trans)} transaction items from {len(df_trans_list)} chunks.\")\n",
    "        del df_trans_list # Free memory\n",
    "\n",
    "    except KeyError as e:\n",
    "        raise KeyError(f\"Error loading transactions: Column '{e}' not found. Required columns are {required_cols}. Please check {transactions_file}.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading transactions file {transactions_file}: {e}\")\n",
    "\n",
    "\n",
    "    category_lookup = None\n",
    "    category_map = None\n",
    "    reverse_category_map = None\n",
    "    num_categories = 0\n",
    "    machine_map = None\n",
    "    reverse_machine_map = None\n",
    "    num_machines = 0\n",
    "\n",
    "    # --- Category Mapping ---\n",
    "    print(\"  Creating category mappings...\")\n",
    "    unique_categories = sorted(df_trans['prod_category_id'].unique())\n",
    "    if not unique_categories:\n",
    "        raise ValueError(\"No valid product categories found in the transactions data.\")\n",
    "\n",
    "    num_categories = len(unique_categories)\n",
    "    category_map = {cat_id: idx for idx, cat_id in enumerate(unique_categories)}\n",
    "    reverse_category_map = {idx: cat_id for cat_id, idx in category_map.items()}\n",
    "    print(f\"Found {num_categories} unique product categories.\")\n",
    "\n",
    "    # --- Machine Mapping ---\n",
    "    print(\"  Creating machine mappings...\")\n",
    "    unique_machines = sorted(df_trans['machine_id'].unique())\n",
    "    if not unique_machines:\n",
    "        raise ValueError(\"No valid machine IDs found in the transactions data.\")\n",
    "\n",
    "    num_machines = len(unique_machines)\n",
    "    machine_map = {mach_id: idx for idx, mach_id in enumerate(unique_machines)}\n",
    "    reverse_machine_map = {idx: mach_id for mach_id, idx in machine_map.items()}\n",
    "    print(f\"Found {num_machines} unique machine IDs.\")\n",
    "\n",
    "\n",
    "    # --- Load Category Names (Optional) ---\n",
    "    if categories_file and os.path.exists(categories_file):\n",
    "        print(f\"  Loading category names from {categories_file}...\")\n",
    "        try:\n",
    "            df_categories = pd.read_csv(\n",
    "                categories_file,\n",
    "                usecols=['Product Category ID', 'Report Category', 'Item Category', 'Item Subcategory']\n",
    "            )\n",
    "            df_categories.rename(columns={'Product Category ID': 'prod_category_id'}, inplace=True)\n",
    "            # Handle potential missing values in name components before concatenation\n",
    "            name_cols = ['Report Category', 'Item Category', 'Item Subcategory']\n",
    "            for col in name_cols:\n",
    "                 if col in df_categories.columns:\n",
    "                     df_categories[col] = df_categories[col].fillna('')\n",
    "                 else:\n",
    "                     print(f\"    Warning: Column '{col}' not found in categories file. It will be skipped for naming.\")\n",
    "                     df_categories[col] = '' # Add empty column if missing to prevent error\n",
    "\n",
    "            df_categories['category_name'] = df_categories['Report Category'] + ' | ' + \\\n",
    "                                             df_categories['Item Category'] + ' | ' + \\\n",
    "                                             df_categories['Item Subcategory']\n",
    "            # Clean up separators if parts are missing\n",
    "            df_categories['category_name'] = df_categories['category_name'].str.replace(r'\\s*\\|\\s*$', '', regex=True).str.replace(r'^\\s*\\|\\s*', '', regex=True).str.replace(r'\\s*\\|\\s*\\|\\s*', ' | ', regex=True).str.strip()\n",
    "            # Create lookup dictionary\n",
    "            category_lookup = df_categories.set_index('prod_category_id')['category_name'].to_dict()\n",
    "            print(f\"    Loaded {len(df_categories)} category definitions.\")\n",
    "        except KeyError as e:\n",
    "            print(f\"    Warning: Column '{e}' not found in categories file {categories_file}. Check column names. Category names might be incomplete.\")\n",
    "            category_lookup = None # Reset lookup if error occurs\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Error loading or processing categories file {categories_file}: {e}\")\n",
    "            category_lookup = None\n",
    "    elif categories_file:\n",
    "         print(f\"  Warning: Categories file specified but not found at {categories_file}. Recommendations will use IDs only.\")\n",
    "    else:\n",
    "         print(f\"  Info: Categories file not specified. Recommendations will use IDs only.\")\n",
    "\n",
    "\n",
    "    print(f\"Data loading finished in {time.time() - start_time:.2f} seconds.\")\n",
    "    return (df_trans,\n",
    "            category_map, reverse_category_map, category_lookup, num_categories,\n",
    "            machine_map, reverse_machine_map, num_machines)\n",
    "\n",
    "def prepare_training_data(df_trans, category_map, machine_map, test_size=0.2, neg_samples=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Groups transactions by session (machine_id, site_session_id), splits sessions\n",
    "    into train/test sets, and generates positive/negative (machine, category, category)\n",
    "    tuples for training.\n",
    "    \"\"\"\n",
    "    print(\"\\nPreparing training data...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Map categories and machines to indices\n",
    "    if not category_map:\n",
    "        raise ValueError(\"Category map is empty or None. Cannot proceed.\")\n",
    "    if not machine_map:\n",
    "        raise ValueError(\"Machine map is empty or None. Cannot proceed.\")\n",
    "\n",
    "    print(\"Step 1: Mapping category and machine IDs to indices...\")\n",
    "    df_trans['category_idx'] = df_trans['prod_category_id'].map(category_map)\n",
    "    df_trans['machine_idx'] = df_trans['machine_id'].map(machine_map)\n",
    "\n",
    "    rows_before_drop = len(df_trans)\n",
    "    # Drop rows where mapping failed for either category or machine\n",
    "    df_trans.dropna(subset=['category_idx', 'machine_idx'], inplace=True)\n",
    "    rows_after_drop = len(df_trans)\n",
    "    if rows_before_drop > rows_after_drop:\n",
    "        print(f\"  Warning: Dropped {rows_before_drop - rows_after_drop} transaction rows due to missing category or machine index mapping.\")\n",
    "\n",
    "    if df_trans.empty:\n",
    "         raise ValueError(\"No transactions remaining after category and machine index mapping.\")\n",
    "\n",
    "    df_trans['category_idx'] = df_trans['category_idx'].astype(int)\n",
    "    df_trans['machine_idx'] = df_trans['machine_idx'].astype(int)\n",
    "\n",
    "\n",
    "    # 2. Group by session (machine_id, site_session_id) and get unique category indices\n",
    "    #    Keep machine_idx associated with the session.\n",
    "    print(\"Step 2: Grouping transactions by session identifier (machine_id, site_session_id)...\")\n",
    "    # Grouping by both machine_id and site_session_id defines a unique session context\n",
    "    # We also need the machine index for each session. Since machine_id is part of the group key,\n",
    "    # we can get it from there.\n",
    "\n",
    "    # Aggregate unique categories and also get the first machine_idx for each group\n",
    "    # (since machine_idx is constant within a session group)\n",
    "    session_data = df_trans.groupby(['machine_id', 'site_session_id'], observed=True, sort=False).agg(\n",
    "        categories=('category_idx', lambda x: list(x.unique())),\n",
    "        machine_idx=('machine_idx', 'first') # Get the single machine index for the session\n",
    "    )\n",
    "\n",
    "    # Filter out sessions with only one unique category type purchased\n",
    "    session_data = session_data[session_data['categories'].apply(len) > 1]\n",
    "    num_multi_item_sessions = len(session_data)\n",
    "    print(f\"  Found {num_multi_item_sessions} sessions with purchases of multiple distinct categories.\")\n",
    "\n",
    "    if session_data.empty:\n",
    "        raise ValueError(\"No sessions found with multiple distinct categories purchased. Cannot generate training pairs.\")\n",
    "\n",
    "    # 3. Split Sessions into Train/Test\n",
    "    print(f\"Step 3: Splitting {num_multi_item_sessions} sessions into train/test ({1-test_size:.0%}/{test_size:.0%})...\")\n",
    "    session_ids = session_data.index.tolist() # List of (machine_id, site_session_id) tuples\n",
    "    try:\n",
    "        # Ensure stratification is possible if test_size is small and num_multi_item_sessions is small\n",
    "        stratify_param = None # No stratification needed here usually\n",
    "        min_sessions_for_split = 2 # Need at least one for train and one for test potentially\n",
    "        if len(session_ids) < min_sessions_for_split:\n",
    "             raise ValueError(f\"Too few sessions ({len(session_ids)}) with multiple items to perform train/test split.\")\n",
    "\n",
    "        train_session_ids, test_session_ids = train_test_split(\n",
    "            session_ids,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            stratify=stratify_param\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error during train/test split: {e}. Check if there are enough sessions ({len(session_ids)} found).\")\n",
    "\n",
    "    train_sessions_data = session_data.loc[train_session_ids]\n",
    "    test_sessions_data = session_data.loc[test_session_ids]\n",
    "    print(f\"  Train sessions: {len(train_sessions_data)}, Test sessions: {len(test_sessions_data)}\")\n",
    "\n",
    "\n",
    "    # 4. Generate Positive and Negative Pairs (including machine index)\n",
    "    print(f\"Step 4: Generating positive and negative tuples (k={neg_samples})...\")\n",
    "    all_category_indices_list = list(category_map.values()) # Use list for np.random.choice\n",
    "    if not all_category_indices_list:\n",
    "         raise ValueError(\"List of all category indices is empty.\")\n",
    "\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "\n",
    "    # --- Inner function for tuple generation ---\n",
    "    def generate_tuples_for_sessions(sessions_df, all_indices_list, k):\n",
    "        data = []\n",
    "        processed_count = 0\n",
    "        total_count = len(sessions_df)\n",
    "        start_pair_gen = time.time()\n",
    "        print(f\"    Generating tuples for {total_count} sessions...\")\n",
    "\n",
    "        # Iterate through rows (each row is a session with its machine_idx and categories)\n",
    "        for _, session_row in sessions_df.iterrows():\n",
    "            machine_idx = session_row['machine_idx']\n",
    "            categories_in_session = session_row['categories']\n",
    "\n",
    "            # Ensure indices are integers and valid\n",
    "            session_indices = [int(idx) for idx in categories_in_session if pd.notna(idx)]\n",
    "            if len(session_indices) < 2:\n",
    "                continue # Skip if not enough valid indices after cleaning\n",
    "\n",
    "            session_cat_set = set(session_indices)\n",
    "\n",
    "            # Positive pairs (co-purchased categories within the session)\n",
    "            # Using permutations captures direction if needed, combinations is faster if symmetric\n",
    "            positive_pairs = list(permutations(session_indices, 2))\n",
    "            for anchor_idx, context_idx in positive_pairs:\n",
    "                data.append((machine_idx, anchor_idx, context_idx, 1)) # Label 1 for positive\n",
    "\n",
    "                # Negative Sampling: Sample categories *not* purchased in this session\n",
    "                neg_count = 0\n",
    "                max_attempts = k * 10 + 20 # Increased attempts limit\n",
    "                attempts = 0\n",
    "                while neg_count < k and attempts < max_attempts:\n",
    "                    # Sample random category index from all possible categories\n",
    "                    negative_idx = np.random.choice(all_indices_list)\n",
    "                    if negative_idx not in session_cat_set: # Check if it wasn't in *this* session\n",
    "                        data.append((machine_idx, anchor_idx, negative_idx, 0)) # Label 0 for negative\n",
    "                        neg_count += 1\n",
    "                    attempts += 1\n",
    "                # Optional: Warning if not enough negative samples found\n",
    "                # if neg_count < k:\n",
    "                #     warnings.warn(f\"Could only generate {neg_count}/{k} negative samples for a session (size {len(session_cat_set)}). May indicate sparse data or very large sessions.\")\n",
    "\n",
    "            processed_count += 1\n",
    "            # Print progress update periodically\n",
    "            if processed_count % (max(1, total_count // 20)) == 0 or processed_count == total_count: # Print ~20 times\n",
    "                elapsed = time.time() - start_pair_gen\n",
    "                rate = processed_count / elapsed if elapsed > 0 else 0\n",
    "                eta = (total_count - processed_count) / rate if rate > 0 else 0\n",
    "                print(f\"\\r      Processed {processed_count}/{total_count} sessions... ({rate:.1f} sess/sec, ETA: {eta:.1f}s)\", end=\"\")\n",
    "\n",
    "        print() # Newline after progress bar\n",
    "        return data\n",
    "    # --- End inner function ---\n",
    "\n",
    "    print(\"  Generating train tuples...\")\n",
    "    train_data = generate_tuples_for_sessions(train_sessions_data, all_category_indices_list, neg_samples)\n",
    "    print(\"  Generating test tuples...\")\n",
    "    test_data = generate_tuples_for_sessions(test_sessions_data, all_category_indices_list, neg_samples)\n",
    "\n",
    "\n",
    "    print(f\"Data preparation finished in {time.time() - start_time:.2f} seconds.\")\n",
    "    print(f\"Generated {len(train_data)} training examples and {len(test_data)} testing examples.\")\n",
    "\n",
    "    # Free up memory by deleting intermediate objects\n",
    "    del df_trans, session_data, train_sessions_data, test_sessions_data\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def get_category_name(cat_id, lookup):\n",
    "    \"\"\"Returns the category name from the lookup, or the ID if not found.\"\"\"\n",
    "    if lookup is None:\n",
    "        return f\"ID: {cat_id}\"\n",
    "    # Ensure cat_id is comparable to keys in lookup (might be int/str)\n",
    "    try:\n",
    "        # Attempt direct lookup first (assuming keys are integers)\n",
    "        return lookup.get(int(cat_id), f\"ID: {cat_id} (Name not found)\")\n",
    "    except (ValueError, TypeError):\n",
    "        # Fallback to string conversion if direct lookup fails or ID is not int-like\n",
    "         return lookup.get(str(cat_id), f\"ID: {cat_id} (Name not found)\")\n",
    "\n",
    "\n",
    "# --- PyTorch Dataset ---\n",
    "class MachineCoOccurrenceDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initializes dataset with list of (machine_idx, anchor_idx, context_idx, label) tuples.\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            warnings.warn(\"MachineCoOccurrenceDataset initialized with empty data.\", RuntimeWarning)\n",
    "            # Initialize empty tensors with correct types\n",
    "            self.machines = torch.empty(0, dtype=torch.long)\n",
    "            self.anchors = torch.empty(0, dtype=torch.long)\n",
    "            self.contexts = torch.empty(0, dtype=torch.long)\n",
    "            self.labels = torch.empty(0, dtype=torch.float)\n",
    "        else:\n",
    "            # Convert list of tuples into separate lists/arrays first for efficiency\n",
    "            machines_list = [item[0] for item in data]\n",
    "            anchors_list = [item[1] for item in data]\n",
    "            contexts_list = [item[2] for item in data]\n",
    "            labels_list = [item[3] for item in data]\n",
    "            # Convert to tensors\n",
    "            self.machines = torch.tensor(machines_list, dtype=torch.long)\n",
    "            self.anchors = torch.tensor(anchors_list, dtype=torch.long)\n",
    "            self.contexts = torch.tensor(contexts_list, dtype=torch.long)\n",
    "            self.labels = torch.tensor(labels_list, dtype=torch.float) # Loss function expects float label\n",
    "            # Clear intermediate lists\n",
    "            del machines_list, anchors_list, contexts_list, labels_list\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return self.labels.shape[0] # Use shape[0] which is reliable even for empty tensors\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns the sample at the given index.\"\"\"\n",
    "        return self.machines[idx], self.anchors[idx], self.contexts[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# --- PyTorch Model ---\n",
    "class MachineCategoryBundleModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding-based model to predict category co-occurrence, incorporating machine context.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_categories, num_machines, embedding_dim):\n",
    "        super(MachineCategoryBundleModel, self).__init__()\n",
    "        if num_categories <= 0:\n",
    "            raise ValueError(\"Number of categories must be positive.\")\n",
    "        if num_machines <= 0:\n",
    "            raise ValueError(\"Number of machines must be positive.\")\n",
    "        self.num_categories = num_categories\n",
    "        self.num_machines = num_machines\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Embedding layer for anchor categories\n",
    "        self.anchor_embedding = nn.Embedding(num_categories, embedding_dim)\n",
    "        # Embedding layer for context categories (can be same or different)\n",
    "        self.context_embedding = nn.Embedding(num_categories, embedding_dim)\n",
    "        # Embedding layer for machines\n",
    "        self.machine_embedding = nn.Embedding(num_machines, embedding_dim)\n",
    "\n",
    "        self.init_weights() # Initialize weights\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initializes embedding weights.\"\"\"\n",
    "        initrange = 0.1 / self.embedding_dim # Scale init range by embedding dim\n",
    "        nn.init.uniform_(self.anchor_embedding.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.context_embedding.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.machine_embedding.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, machine_idx, anchor_cat_idx, context_cat_idx):\n",
    "        \"\"\"\n",
    "        Performs the forward pass.\n",
    "        Args:\n",
    "            machine_idx (Tensor): Tensor of machine indices (batch_size).\n",
    "            anchor_cat_idx (Tensor): Tensor of anchor category indices (batch_size).\n",
    "            context_cat_idx (Tensor): Tensor of context category indices (batch_size).\n",
    "        Returns:\n",
    "            Tensor: Predicted co-occurrence scores (logits) (batch_size).\n",
    "        \"\"\"\n",
    "        # Look up embeddings: shape (batch_size, embedding_dim)\n",
    "        machine_embeds = self.machine_embedding(machine_idx)\n",
    "        anchor_embeds = self.anchor_embedding(anchor_cat_idx)\n",
    "        context_embeds = self.context_embedding(context_cat_idx)\n",
    "\n",
    "        # Combine machine and anchor embeddings (simple addition)\n",
    "        # Other options: concatenation + linear layer, element-wise product\n",
    "        combined_anchor_embeds = anchor_embeds + machine_embeds\n",
    "\n",
    "        # Compute dot product score between combined anchor and context: shape (batch_size)\n",
    "        # score = torch.einsum('be,be->b', combined_anchor_embeds, context_embeds)\n",
    "        score = torch.sum(combined_anchor_embeds * context_embeds, dim=1)\n",
    "\n",
    "        return score # Return logits (raw scores before sigmoid)\n",
    "\n",
    "# --- Training Function ---\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"Trains the model and evaluates it on the test set after each epoch.\"\"\"\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    model.to(device) # Move model to the specified device\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    total_batches_train = len(train_loader)\n",
    "    total_batches_test = len(test_loader)\n",
    "\n",
    "    if total_batches_train == 0:\n",
    "         print(\"Warning: Train loader is empty. Skipping training loop.\")\n",
    "         return [], [], []\n",
    "\n",
    "    print(f\"Training for {num_epochs} epochs with {total_batches_train} train batches and {total_batches_test} test batches per epoch.\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        start_time_epoch = time.time()\n",
    "\n",
    "        for i, (machine_idx, anchor_idx, context_idx, labels) in enumerate(train_loader):\n",
    "            # Move batch data to the device\n",
    "            machine_idx = machine_idx.to(device, non_blocking=True)\n",
    "            anchor_idx = anchor_idx.to(device, non_blocking=True)\n",
    "            context_idx = context_idx.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(machine_idx, anchor_idx, context_idx) # Pass machine_idx too\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Check for NaN/Inf loss and skip update if found\n",
    "            if not torch.isfinite(loss):\n",
    "                warnings.warn(f\"Warning: Non-finite loss detected at epoch {epoch+1}, batch {i+1}. Skipping batch update.\", RuntimeWarning)\n",
    "                continue\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            # Optional: Gradient clipping (helps prevent exploding gradients)\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Print progress periodically\n",
    "            if (i + 1) % (max(1, total_batches_train // 10)) == 0: # Print ~10 times per epoch\n",
    "                print(f'\\rEpoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_batches_train}], Batch Loss: {loss.item():.4f}', end=\"\")\n",
    "\n",
    "        # Calculate average loss for the epoch\n",
    "        epoch_loss = running_loss / total_batches_train if total_batches_train > 0 else 0.0\n",
    "        train_losses.append(epoch_loss)\n",
    "        epoch_time = time.time() - start_time_epoch\n",
    "        print() # Newline after epoch progress\n",
    "\n",
    "        # --- Evaluation Phase ---\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        current_test_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        if total_batches_test == 0:\n",
    "            print(\"  Warning: Test loader is empty. Skipping evaluation for this epoch.\")\n",
    "            avg_test_loss = float('nan') # Indicate missing evaluation\n",
    "            accuracy = float('nan')\n",
    "        else:\n",
    "            with torch.no_grad(): # Disable gradient calculations for evaluation\n",
    "                for machine_idx, anchor_idx, context_idx, labels in test_loader:\n",
    "                    # Move batch data to the device\n",
    "                    machine_idx = machine_idx.to(device, non_blocking=True)\n",
    "                    anchor_idx = anchor_idx.to(device, non_blocking=True)\n",
    "                    context_idx = context_idx.to(device, non_blocking=True)\n",
    "                    labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = model(machine_idx, anchor_idx, context_idx) # Pass machine_idx\n",
    "\n",
    "                    # Calculate loss\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Accumulate test loss, checking for non-finite values\n",
    "                    if torch.isfinite(loss):\n",
    "                        current_test_loss += loss.item()\n",
    "                    else:\n",
    "                        warnings.warn(f\"Warning: Non-finite loss encountered during evaluation epoch {epoch+1}.\", RuntimeWarning)\n",
    "\n",
    "\n",
    "                    # Calculate accuracy\n",
    "                    # Apply sigmoid to get probabilities, then threshold at 0.5\n",
    "                    predicted = torch.sigmoid(outputs) >= 0.5\n",
    "                    total_predictions += labels.size(0)\n",
    "                    correct_predictions += (predicted.byte() == labels.byte()).sum().item() # Ensure types match for comparison\n",
    "\n",
    "            # Calculate average test loss and accuracy\n",
    "            avg_test_loss = current_test_loss / total_batches_test if total_batches_test > 0 else float('nan')\n",
    "            accuracy = (correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0.0\n",
    "\n",
    "        test_losses.append(avg_test_loss)\n",
    "        test_accuracies.append(accuracy)\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.2f}s.')\n",
    "        print(f'  Avg Train Loss: {epoch_loss:.4f}')\n",
    "        print(f'  Avg Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    print(\"--- Training Finished ---\")\n",
    "    return train_losses, test_losses, test_accuracies\n",
    "\n",
    "# --- Recommendation Function ---\n",
    "def recommend_bundles_pytorch(input_machine_id, input_category_id, model,\n",
    "                              machine_map, category_map, reverse_category_map,\n",
    "                              num_categories, num_recommendations, device, category_lookup):\n",
    "    \"\"\"\n",
    "    Recommends product categories to bundle with the input category for a specific machine,\n",
    "    using the trained model.\n",
    "\n",
    "    Args:\n",
    "        input_machine_id (int): The original ID of the machine to get recommendations for.\n",
    "        input_category_id (int): The original ID of the category to get recommendations for.\n",
    "        model (nn.Module): The trained MachineCategoryBundleModel.\n",
    "        machine_map (dict): Mapping from original machine ID to model index.\n",
    "        category_map (dict): Mapping from original category ID to model index.\n",
    "        reverse_category_map (dict): Mapping from model index to original category ID.\n",
    "        num_categories (int): Total number of unique categories the model was trained on.\n",
    "        num_recommendations (int): The number of recommendations to return.\n",
    "        device (torch.device): The device (CPU or CUDA) to run inference on.\n",
    "        category_lookup (dict, optional): Mapping from original category ID to category name.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of recommended original category IDs.\n",
    "    \"\"\"\n",
    "    model.eval() # Ensure model is in evaluation mode\n",
    "    model.to(device) # Move model to the correct device\n",
    "\n",
    "    # Validate inputs\n",
    "    if input_machine_id not in machine_map:\n",
    "        print(f\"\\nError: Input machine ID {input_machine_id} was not found in the model's machine map (likely not seen in training data).\")\n",
    "        print(f\"Available machine IDs in map: {list(machine_map.keys())[:10]}...\") # Show a few examples\n",
    "        return []\n",
    "    if input_category_id not in category_map:\n",
    "        print(f\"\\nError: Input category ID {input_category_id} was not found in the model's category map (likely not seen in training data).\")\n",
    "        print(f\"Available category IDs in map: {list(category_map.keys())[:10]}...\") # Show a few examples\n",
    "        return []\n",
    "\n",
    "    input_machine_idx = machine_map[input_machine_id]\n",
    "    input_category_idx = category_map[input_category_id]\n",
    "\n",
    "    print(f\"\\nGenerating recommendations for Machine ID: {input_machine_id} (Index: {input_machine_idx})\")\n",
    "    print(f\"  and Category: {get_category_name(input_category_id, category_lookup)} (ID: {input_category_id}, Index: {input_category_idx})\")\n",
    "\n",
    "    # Prepare input tensors for the machine and anchor category (repeated for batching)\n",
    "    machine_idx_tensor = torch.tensor([input_machine_idx], dtype=torch.long).to(device)\n",
    "    anchor_idx_tensor = torch.tensor([input_category_idx], dtype=torch.long).to(device)\n",
    "\n",
    "    # Prepare input tensor for all possible context categories\n",
    "    all_context_indices = torch.arange(num_categories, dtype=torch.long).to(device)\n",
    "\n",
    "    # Predict scores in batches to manage memory if num_categories is large\n",
    "    all_scores = []\n",
    "    eval_batch_size = 8192 # Adjust based on GPU memory and num_categories\n",
    "    print(f\"  Predicting scores against {num_categories} potential categories...\")\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        for i in range(0, num_categories, eval_batch_size):\n",
    "            # Create batch for context indices\n",
    "            batch_context = all_context_indices[i : i + eval_batch_size]\n",
    "            batch_size = len(batch_context)\n",
    "\n",
    "            # Repeat the machine and anchor index for the batch size\n",
    "            batch_machine = machine_idx_tensor.repeat(batch_size)\n",
    "            batch_anchor = anchor_idx_tensor.repeat(batch_size)\n",
    "\n",
    "            # Get scores for the batch\n",
    "            batch_scores = model(batch_machine, batch_anchor, batch_context)\n",
    "            all_scores.append(batch_scores.cpu()) # Move scores to CPU immediately\n",
    "\n",
    "    # Concatenate scores from all batches and convert to numpy array\n",
    "    scores = torch.cat(all_scores).numpy()\n",
    "    print(f\"  Finished predicting scores.\")\n",
    "\n",
    "    # Create pairs of (original_category_id, score)\n",
    "    category_scores = []\n",
    "    for i in range(num_categories):\n",
    "        # Exclude the input category itself from recommendations\n",
    "        if i != input_category_idx:\n",
    "            original_cat_id = reverse_category_map.get(i, None)\n",
    "            if original_cat_id is not None:\n",
    "                # scores[i] corresponds to the score between (input_machine, input_cat) and context cat i\n",
    "                category_scores.append((original_cat_id, scores[i]))\n",
    "\n",
    "    # Sort categories by predicted score in descending order\n",
    "    category_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get top N recommendations\n",
    "    top_recs = category_scores[:num_recommendations]\n",
    "\n",
    "    print(f\"\\nTop {len(top_recs)} recommended categories to bundle:\")\n",
    "    recommendations = []\n",
    "    if not top_recs:\n",
    "        print(\"  No recommendations found (or all other categories had low predicted scores).\")\n",
    "    else:\n",
    "        for cat_id, score in top_recs:\n",
    "            recommendations.append(cat_id)\n",
    "            print(f\"  - {get_category_name(cat_id, category_lookup)} (ID: {cat_id}) (Score: {score:.4f})\")\n",
    "\n",
    "    return recommendations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5399e23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Product Bundle Recommender Script (with Machine Context)\n",
      "============================================================\n",
      "Using device: cpu\n",
      "Loading data...\n",
      "  Transactions file: transactions.csv\n",
      "  Categories file: product_categories.csv\n",
      "  Processing transaction file in chunks...\n",
      "    Processed chunk 1...\n",
      "    Processed chunk 2...\n",
      "    Processed chunk 3...\n",
      "Loaded 2308972 transaction items from 3 chunks.\n",
      "  Creating category mappings...\n",
      "Found 222 unique product categories.\n",
      "  Creating machine mappings...\n",
      "Found 103347 unique machine IDs.\n",
      "  Loading category names from product_categories.csv...\n",
      "    Warning: Error loading or processing categories file product_categories.csv: Usecols do not match columns, columns expected but not found: ['Item Subcategory']\n",
      "Data loading finished in 5.78 seconds.\n",
      "\n",
      "Preparing training data...\n",
      "Step 1: Mapping category and machine IDs to indices...\n",
      "Step 2: Grouping transactions by session identifier (machine_id, site_session_id)...\n",
      "  Found 205802 sessions with purchases of multiple distinct categories.\n",
      "Step 3: Splitting 205802 sessions into train/test (80%/20%)...\n",
      "  Train sessions: 164641, Test sessions: 41161\n",
      "Step 4: Generating positive and negative tuples (k=5)...\n",
      "  Generating train tuples...\n",
      "    Generating tuples for 164641 sessions...\n",
      "      Processed 164641/164641 sessions... (341.4 sess/sec, ETA: 0.0s)))\n",
      "  Generating test tuples...\n",
      "    Generating tuples for 41161 sessions...\n",
      "      Processed 41161/41161 sessions... (322.2 sess/sec, ETA: 0.0s))\n",
      "Data preparation finished in 622.83 seconds.\n",
      "Generated 36472848 training examples and 9264348 testing examples.\n",
      "\n",
      "Creating Datasets and DataLoaders...\n",
      "  Using 0 workers for DataLoaders.\n",
      "  DataLoaders created in 21.69 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================\n",
    "# --- Main Script ---\n",
    "# ===================\n",
    "\n",
    "print(\"Starting Product Bundle Recommender Script (with Machine Context)\")\n",
    "print(\"=\"*60)\n",
    "overall_start_time = time.time()\n",
    "\n",
    "# 0. Setup Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "        print(f\"  CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# 1. Load Data and Mappings\n",
    "# This step reads transactions.csv and prepares category AND machine mappings\n",
    "(df_transactions,\n",
    " cat_map, rev_cat_map, cat_lookup, n_categories,\n",
    " mach_map, rev_mach_map, n_machines) = load_data(\n",
    "    TRANSACTIONS_FILE, CATEGORIES_FILE\n",
    ")\n",
    "\n",
    "# 2. Prepare Data for Training\n",
    "# This step groups transactions by session, splits sessions, and generates (machine, anchor, context, label) tuples\n",
    "train_tuples, test_tuples = prepare_training_data(\n",
    "    df_trans=df_transactions, # Pass the loaded dataframe\n",
    "    category_map=cat_map,\n",
    "    machine_map=mach_map, # Pass the machine map\n",
    "    test_size=TEST_SET_SIZE,\n",
    "    neg_samples=NEGATIVE_SAMPLES_PER_POSITIVE,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "# Clear the large transactions dataframe now that tuples are generated\n",
    "del df_transactions\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Check if data generation was successful\n",
    "if not train_tuples:\n",
    "        print(\"\\nError: No training data tuples were generated. This might happen if there are no sessions with multiple category purchases.\")\n",
    "        exit(1)\n",
    "\n",
    "# 3. Create Datasets and DataLoaders\n",
    "print(\"\\nCreating Datasets and DataLoaders...\")\n",
    "start_dataloader_time = time.time()\n",
    "train_dataset = MachineCoOccurrenceDataset(train_tuples) # Use the new dataset class\n",
    "test_dataset = MachineCoOccurrenceDataset(test_tuples)\n",
    "# Clear tuple lists to save memory\n",
    "del train_tuples, test_tuples\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Determine num_workers based on platform/device\n",
    "num_workers = 0 # Often safer default, especially with GPU and Windows\n",
    "# Adjust based on your system's capabilities and testing\n",
    "# if device.type == 'cpu' and os.name != 'nt':\n",
    "#       num_workers = max(1, os.cpu_count() // 2) # Example heuristic for CPU on non-Windows\n",
    "\n",
    "print(f\"  Using {num_workers} workers for DataLoaders.\")\n",
    "pin_memory_flag = True if device.type == 'cuda' else False\n",
    "\n",
    "# Set persistent_workers based on num_workers\n",
    "persist_workers_flag = False if num_workers == 0 else True\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=pin_memory_flag, persistent_workers=persist_workers_flag)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory_flag, persistent_workers=persist_workers_flag)\n",
    "print(f\"  DataLoaders created in {time.time() - start_dataloader_time:.2f} seconds.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c46e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Model, Loss Function, and Optimizer...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MachineCategoryBundleModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 4. Initialize Model, Loss, Optimizer\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mInitializing Model, Loss Function, and Optimizer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mMachineCategoryBundleModel\u001b[49m( \u001b[38;5;66;03m# Use the new model class\u001b[39;00m\n\u001b[32m      6\u001b[39m     num_categories=n_categories,\n\u001b[32m      7\u001b[39m     num_machines=n_machines, \u001b[38;5;66;03m# Pass the number of machines\u001b[39;00m\n\u001b[32m      8\u001b[39m     embedding_dim=EMBEDDING_DIM\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m criterion = nn.BCEWithLogitsLoss() \u001b[38;5;66;03m# Numerically stable loss for binary classification with logits\u001b[39;00m\n\u001b[32m     11\u001b[39m optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
      "\u001b[31mNameError\u001b[39m: name 'MachineCategoryBundleModel' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# 4. Initialize Model, Loss, Optimizer\n",
    "print(\"\\nInitializing Model, Loss Function, and Optimizer...\")\n",
    "model = MachineCategoryBundleModel( # Use the new model class\n",
    "    num_categories=n_categories,\n",
    "    num_machines=n_machines, # Pass the number of machines\n",
    "    embedding_dim=EMBEDDING_DIM\n",
    ")\n",
    "criterion = nn.BCEWithLogitsLoss() # Numerically stable loss for binary classification with logits\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "print(f\"  Model: {model.__class__.__name__} with {EMBEDDING_DIM}-dim embeddings for {n_categories} categories and {n_machines} machines.\")\n",
    "print(f\"  Optimizer: Adam (LR={LEARNING_RATE})\")\n",
    "print(f\"  Loss Function: BCEWithLogitsLoss\")\n",
    "\n",
    "\n",
    "# 5. Train the Model and get loss history\n",
    "train_loss_history, test_loss_history, test_accuracy_history = train_model(\n",
    "    model, train_loader, test_loader, criterion, optimizer, NUM_EPOCHS, device\n",
    ")\n",
    "\n",
    "# --- Plotting ---\n",
    "if train_loss_history or test_loss_history: # Only plot if training actually ran\n",
    "    print(\"\\n--- Plotting Training and Test Curves ---\")\n",
    "    epochs = range(1, len(train_loss_history) + 1)\n",
    "    plt.figure(figsize=(12, 5.5)) # Slightly adjusted figure size\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss_history, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs, test_loss_history, 'ro-', label='Test Loss')\n",
    "    plt.title('Training and Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (BCEWithLogits)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    if test_accuracy_history: # Only plot if accuracy was calculated\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, test_accuracy_history, 'go-', label='Test Accuracy')\n",
    "        plt.title('Test Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_curves_machine_model.png\") # Save the plot\n",
    "    print(\"Saved training curves plot to 'training_curves_machine_model.png'\")\n",
    "    # plt.show() # Optionally display plot\n",
    "\n",
    "\n",
    "# --- Example Recommendation ---\n",
    "print(\"\\n--- Generating Example Recommendation ---\")\n",
    "if not mach_map:\n",
    "    print(\"Machine map is empty, cannot generate recommendations.\")\n",
    "elif not cat_map:\n",
    "    print(\"Category map is empty, cannot generate recommendations.\")\n",
    "else:\n",
    "    # Get a random machine ID and category ID from the training data maps for demonstration\n",
    "    try:\n",
    "        # Pick a machine ID that was actually used in training/mapping\n",
    "        available_machine_ids = list(mach_map.keys())\n",
    "        if not available_machine_ids:\n",
    "             raise ValueError(\"Machine map keys are empty.\")\n",
    "        target_machine_id = random.choice(available_machine_ids)\n",
    "\n",
    "        # Pick a category ID that was actually used in training/mapping\n",
    "        available_category_ids = list(cat_map.keys())\n",
    "        if not available_category_ids:\n",
    "             raise ValueError(\"Category map keys are empty.\")\n",
    "        target_category_id = random.choice(available_category_ids)\n",
    "\n",
    "        print(f\"Selected sample machine ID: {target_machine_id}\")\n",
    "        print(f\"Selected sample category ID: {target_category_id}\")\n",
    "\n",
    "        # Generate recommendations\n",
    "        recommended_ids = recommend_bundles_pytorch(\n",
    "            input_machine_id=target_machine_id,\n",
    "            input_category_id=target_category_id,\n",
    "            model=model,\n",
    "            machine_map=mach_map,\n",
    "            category_map=cat_map,\n",
    "            reverse_category_map=rev_cat_map,\n",
    "            num_categories=n_categories,\n",
    "            num_recommendations=RECOMMENDATION_COUNT,\n",
    "            device=device,\n",
    "            category_lookup=cat_lookup\n",
    "        )\n",
    "\n",
    "    except IndexError:\n",
    "         print(\"Could not select a random machine/category ID (maps might be empty after all).\")\n",
    "    except ValueError as e:\n",
    "         print(f\"Error selecting sample IDs: {e}\")\n",
    "    except Exception as e:\n",
    "         print(f\"An unexpected error occurred during recommendation generation: {e}\")\n",
    "\n",
    "\n",
    "# --- Optional: Save Model ---\n",
    "# print(\"\\n--- Saving Model ---\")\n",
    "# model_save_path = \"machine_category_recommender_model.pth\"\n",
    "# try:\n",
    "#     torch.save(model.state_dict(), model_save_path)\n",
    "#     print(f\"Model state dictionary saved to {model_save_path}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error saving model: {e}\")\n",
    "\n",
    "\n",
    "overall_end_time = time.time()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Script finished in {overall_end_time - overall_start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5bf276",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_transactions,\n",
    " cat_map, rev_cat_map, cat_lookup, n_categories,\n",
    " mach_map, rev_mach_map, n_machines) = load_data(\n",
    "    TRANSACTIONS_FILE, CATEGORIES_FILE\n",
    ")\n",
    "\n",
    "\n",
    "recommended_ids = recommend_bundles_pytorch(\n",
    "    input_machine_id=target_machine_id,\n",
    "    input_category_id=target_category_id,\n",
    "    model=model,\n",
    "    machine_map=mach_map,\n",
    "    category_map=cat_map,\n",
    "    reverse_category_map=rev_cat_map,\n",
    "    num_categories=n_categories,\n",
    "    num_recommendations=RECOMMENDATION_COUNT,\n",
    "    device=device,\n",
    "    category_lookup=cat_lookup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e67463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
